{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "H0kj-8xxnORC",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "cBFFvTBNJzUa",
        "KH5McJBi2d8v",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Unsupervised ML - Zomato Restaurant Clustering**\n",
        "![zomato.jpeg](data:image/jpeg;base64,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)\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ktivAwHHrYtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Samiksha Wakode\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is centered on analyzing restaurant data in India to extract insights regarding cuisines, costs, ratings, collections, and sentiments. The methodology encompasses data wrangling, dimensionality reduction, and the application of machine learning models such as K-means and hierarchical clustering. Additionally, topic modeling using LDA and sentiment analysis techniques are utilized. The resulting findings offer valuable information on prevalent cuisines, restaurant affordability, rating trends, and customer sentiments. The report includes recommendations and suggestions for future work to encourage further exploration in this domain."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato is an Indian restaurant aggregator and food delivery start-up founded by Deepinder Goyal and Pankaj Chaddah in 2008. Zomato provides information, menus and user-reviews of restaurants, and also has food delivery options from partner restaurants in select cities. India is quite famous for its diverse multi cuisine available in a large number of restaurants and hotel resorts, which is reminiscent of unity in diversity. Restaurant business in India is always evolving. More Indians are warming up to the idea of eating restaurant food whether by dining outside or getting food delivered. The growing number of restaurants in every state of India has been a motivation to inspect the data to get some insights, interesting facts and figures about the Indian food industry in each city. So, this project focuses on analysing the Zomato restaurant data for each city in India.\n",
        "\n",
        "The Project focuses on Customers and Company, you have to analyze the sentiments of the reviews given by the customer in the data and make some useful conclusions in the form of Visualizations. Also, cluster the zomato restaurants into different segments. The data is vizualized as it becomes easy to analyse data at instant. The Analysis also solves some of the business cases that can directly help the customers finding the Best restaurant in their locality and for the company to grow up and work on the fields they are currently lagging in. This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#importing random forest and XgB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Non-negative matrix Factorization\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#principal component analysis\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "#importing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "#for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# for POS tagging(Part of speech in NLP sentiment analysis)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#import stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#import tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#LDA\n",
        "#!pip install pyldavis\n",
        "#import pyldavis.sklearn\n",
        "#from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "#importing contraction\n",
        "!pip install contractions\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "#importing shap for model explainability\n",
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "#download small spacy model\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = \"/content/drive/MyDrive/Zomato Restaurant names and Metadata.csv\"\n",
        "hotel_df = pd.read_csv(path1)"
      ],
      "metadata": {
        "id": "NqT6mjCtTUlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path2 = \"/content/drive/MyDrive/Zomato Restaurant reviews.csv\"\n",
        "review_df = pd.read_csv(path2)"
      ],
      "metadata": {
        "id": "LuEggHI6TUh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look restaurant\n",
        "hotel_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look review\n",
        "review_df.head()\n"
      ],
      "metadata": {
        "id": "MvVxSUI6TsJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows(Observation) & Columns count(Feature)\n",
        "print(f'Total observation and feature for hotel: {hotel_df.shape}')\n",
        "print(f'Total observation and feature for review: {review_df.shape}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print('Restaurant Info')\n",
        "print('\\n')\n",
        "hotel_df.info()\n",
        "print('='*120)\n",
        "print('\\n')\n",
        "print('Review Info')\n",
        "print('\\n')\n",
        "review_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print('For Restaurant')\n",
        "print('\\n')\n",
        "print(f\"unique values with {len(hotel_df[hotel_df.duplicated()])} duplication\")\n",
        "print('\\n')\n",
        "print('='*120)\n",
        "print('\\n')\n",
        "print('For Reviews')\n",
        "print('\\n')\n",
        "print(f\"unique values with {len(review_df[review_df.duplicated()])} duplication\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting duplicate values\n",
        "print(f' Duplicate data count = {review_df[review_df.duplicated()].shape[0]}')\n",
        "review_df[review_df.duplicated()]"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking values for Anerican Wild Things\n",
        "review_df[(review_df['Restaurant'] == 'American Wild Wings')].shape"
      ],
      "metadata": {
        "id": "pX0Zd4z3UIg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking values for Arena Eleven\n",
        "review_df[(review_df['Restaurant'] == 'Arena Eleven')].shape\n"
      ],
      "metadata": {
        "id": "z0naEckrULZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count for hotel data\n",
        "hotel_df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count for review data\n",
        "review_df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "KqW2T6H5UWaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values for restaurant\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(hotel_df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "msnE7uGEUZZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values for reviews\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(review_df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "8iHjW7VcUd6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurant DataSet**\n",
        "\n",
        "* There are 105 total observation with 6 different features.\n",
        "\n",
        "* Feature like collection and timing has null values.\n",
        "\n",
        "* There is no duplicate values i.e., 105 unique data.\n",
        "\n",
        "* Feature cost represent amount but has object data type because these values are separated by comma ','.\n",
        "\n",
        "* Timing represent operational hour but as it is represented in the form of text has object data type.\n",
        "\n",
        "\n",
        "**Review DataSet**\n",
        "\n",
        "* There are total 10000 observation and 7 features.\n",
        "\n",
        "* Except picture and restaurant feature all others have null values.\n",
        "\n",
        "* There are total of 36 duplicate values for two restaurant - American Wild Wings and Arena Eleven, where all these duplicate values generally have null values.\n",
        "\n",
        "* Rating represent ordinal data, has object data type should be integer.\n",
        "\n",
        "* Timing represent the time when review was posted but show object data time, it should be converted into date time."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns restaurant\n",
        "print(f'Features : {hotel_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns review\n",
        "print(f'Features : {review_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "yqU31HiMVdO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe restaurant\n",
        "hotel_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe review\n",
        "review_df.describe(include='all').T"
      ],
      "metadata": {
        "id": "CZ_w1VpOV-MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zomato Restaurant**\n",
        "\n",
        "* **Name** : Name of Restaurants\n",
        "\n",
        "* **Links** : URL Links of Restaurants\n",
        "\n",
        "* **Cost** : Per person estimated Cost of dining\n",
        "\n",
        "* **Collection** : Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "* **Cuisines** : Cuisines served by Restaurants\n",
        "\n",
        "* **Timings** : Restaurant Timings\n",
        "\n",
        "**Zomato Restaurant Reviews**\n",
        "\n",
        "* **Restaurant** : Name of the Restaurant\n",
        "\n",
        "* **Reviewer** : Name of the Reviewer\n",
        "\n",
        "* **Review** : Review Text\n",
        "\n",
        "* **Rating** : Rating Provided by Reviewer\n",
        "\n",
        "* **MetaData** : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "* **Time** : Date and Time of Review\n",
        "\n",
        "* **Pictures** : No. of pictures posted with review\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check Unique Values for each variable for Hotels\n",
        "print('Check Unique Values for each variable for Hotels:')\n",
        "print()\n",
        "for i in hotel_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",hotel_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable for reviews\n",
        "print('Check Unique Values for each variable for reviews:')\n",
        "print()\n",
        "for i in review_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",review_df[i].nunique(),\".\")\n"
      ],
      "metadata": {
        "id": "DCzpr2icXGSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy of both the data\n",
        "hotel = hotel_df.copy()\n",
        "review = review_df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resturant**"
      ],
      "metadata": {
        "id": "PJVSHQIfXOj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#before changing data type for cost checking values\n",
        "hotel['Cost'].unique()\n"
      ],
      "metadata": {
        "id": "PQ8nGT7BXTDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# changing the data type of the cost function\n",
        "hotel['Cost'] = hotel['Cost'].replace(\"[^\\d]\",'',regex=True).astype('int64')"
      ],
      "metadata": {
        "id": "VYvBzo-yXXwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 costlier restaurant\n",
        "hotel.sort_values('Cost', ascending = False)[['Name','Cost']][:5]"
      ],
      "metadata": {
        "id": "c0NiBq4CXZp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 economy restaurant\n",
        "hotel.sort_values('Cost', ascending = False)[['Name','Cost']][-5:]"
      ],
      "metadata": {
        "id": "z_Uls4atXd3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotels that share same price\n",
        "hotel_dict = {}\n",
        "amount = hotel.Cost.values.tolist()\n",
        "\n",
        "#adding hotel name based on the price by converting it into list\n",
        "for price in amount:\n",
        "    # Get all the rows that have the current price\n",
        "    rows = hotel[hotel['Cost'] == price]\n",
        "    hotel_dict[price] = rows[\"Name\"].tolist()\n",
        "\n",
        "#converting it into dataframe\n",
        "same_price_hotel_df=pd.DataFrame.from_dict([hotel_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Cost',0:'Name of Restaurants'})\n",
        "\n",
        "#alternate methode to do the same\n",
        "#same_price_hotel_df = hotel.groupby('Cost')['Name'].apply(lambda x: x.tolist()).reset_index()\n",
        "\n",
        "#getting hotel count\n",
        "hotel_count = hotel.groupby('Cost')['Name'].count().reset_index().sort_values(\n",
        "    'Cost', ascending = False)\n",
        "\n",
        "#merging together\n",
        "same_price_hotel_df = same_price_hotel_df.merge(hotel_count, how = 'inner',\n",
        "                        on = 'Cost').rename(columns = {'Name':'Total_Restaurant'})\n",
        "\n",
        "#max hotels that share same price\n",
        "same_price_hotel_df.sort_values('Total_Restaurant', ascending = False)[:5]\n",
        "\n"
      ],
      "metadata": {
        "id": "Vi7Mwu6zXm8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotels which has max price\n",
        "same_price_hotel_df.sort_values('Cost', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "ldhUUBPLXsZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the cusines and storing in list\n",
        "cuisine_value_list = hotel.Cuisines.str.split(', ')"
      ],
      "metadata": {
        "id": "lCQivxl_XwAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines in a dict\n",
        "cuisine_dict = {}\n",
        "for cuisine_names in cuisine_value_list:\n",
        "    for cuisine in cuisine_names:\n",
        "        if (cuisine in cuisine_dict):\n",
        "            cuisine_dict[cuisine]+=1\n",
        "        else:\n",
        "            cuisine_dict[cuisine]=1\n"
      ],
      "metadata": {
        "id": "Igd6t1ZmXz-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "cuisine_df=pd.DataFrame.from_dict([cuisine_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Cuisine',0:'Number of Restaurants'})"
      ],
      "metadata": {
        "id": "wMTJmWyhX2hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 cuisine\n",
        "cuisine_df.sort_values('Number of Restaurants', ascending =False)[:5]"
      ],
      "metadata": {
        "id": "Tvzsx3WgX5pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the cusines and storing in list\n",
        "Collections_value_list = hotel.Collections.dropna().str.split(', ')"
      ],
      "metadata": {
        "id": "y1ciPVraX9eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines in a dict\n",
        "Collections_dict = {}\n",
        "for collection in Collections_value_list:\n",
        "    for col_name in collection:\n",
        "        if (col_name in Collections_dict):\n",
        "            Collections_dict[col_name]+=1\n",
        "        else:\n",
        "            Collections_dict[col_name]=1"
      ],
      "metadata": {
        "id": "8dHLBus6X_Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "Collections_df=pd.DataFrame.from_dict([Collections_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Tags',0:'Number of Restaurants'})"
      ],
      "metadata": {
        "id": "k-nsQvIgYBzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 collection\n",
        "Collections_df.sort_values('Number of Restaurants', ascending =False)[:5]"
      ],
      "metadata": {
        "id": "s5M_K9wUYFlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reviews**"
      ],
      "metadata": {
        "id": "jTvjHqrmYLY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#in order to change data type for rating checking values\n",
        "review.Rating.value_counts()\n",
        "\n"
      ],
      "metadata": {
        "id": "1xMU6JHZYQ1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing data type for each rating since had value as interger surrounded by inverted comma\n",
        "#since there is one rating as like converting it to 0 since no rating is 0 then to median\n",
        "review.loc[review['Rating'] == 'Like'] = 0\n",
        "#changing data type for rating in review data\n",
        "review['Rating'] = review['Rating'].astype('float')"
      ],
      "metadata": {
        "id": "K4UJ9Xk7YS2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since there is one rating as like converting it to median\n",
        "review.loc[review['Rating'] == 0] = review.Rating.median()"
      ],
      "metadata": {
        "id": "JMKm16O0YSzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting 'Metadata' column into two separate columns\n",
        "split_metadata = review['Metadata'].str.split(',', expand=True)\n",
        "\n",
        "# Assigning the split results to separate columns\n",
        "review['Reviewer_Total_Review'] = pd.to_numeric(split_metadata[0].str.split(' ').str[0])\n",
        "review['Reviewer_Followers'] = pd.to_numeric(split_metadata[1].str.split(' ').str[1])\n",
        "try:\n",
        "    review['Time'] = pd.to_datetime(review['Time'], errors='coerce')\n",
        "except ValueError:\n",
        "    # Handle the error, print a message, or perform other actions as needed\n",
        "    print(\"Some values in the 'Time' column are not in a recognized datetime format.\")\n",
        "    # Optionally, you can drop rows with invalid datetime values\n",
        "    # review = review.dropna(subset=['Time'])\n",
        "\n",
        "# Extracting additional features from the 'Time' column\n",
        "review['Review_Year'] = review['Time'].dt.year\n",
        "review['Review_Month'] = review['Time'].dt.month\n",
        "review['Review_Hour'] = review['Time'].dt.hour\n"
      ],
      "metadata": {
        "id": "KB4qE6c2YSw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average engagement of restaurants\n",
        "avg_hotel_rating = review.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})\n",
        "avg_hotel_rating"
      ],
      "metadata": {
        "id": "QONu18o4YSt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usless data\n",
        "review[review['Restaurant'] == 4.0]\n"
      ],
      "metadata": {
        "id": "SOSAeefcYSq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking hotel count as total hotel in restaurant data was 105\n",
        "review.Restaurant.nunique()\n"
      ],
      "metadata": {
        "id": "VHg0xZBdYkwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding hotel without review\n",
        "hotel_without_review = [name for name in hotel.Name.unique().tolist()\n",
        "       if name not in review.Restaurant.unique().tolist()]\n",
        "hotel_without_review"
      ],
      "metadata": {
        "id": "1uqfM2p9YoZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 most engaging or rated restaurant\n",
        "avg_hotel_rating.sort_values('Rating', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "IVW4pvdKYsl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#top 5 lowest rated restaurant\n",
        "avg_hotel_rating.sort_values('Rating', ascending = True)[:5]"
      ],
      "metadata": {
        "id": "2PEKU-lWYuhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the most followed critic\n",
        "most_followed_reviewer = review.groupby('Reviewer').agg({'Reviewer_Total_Review':'max',\n",
        "      'Reviewer_Followers':'max', 'Rating':'mean'}).reset_index().rename(columns = {\n",
        "          'Rating':'Average_Rating_Given'}).sort_values('Reviewer_Followers', ascending = False)\n",
        "most_followed_reviewer[:5]"
      ],
      "metadata": {
        "id": "5iP1mC2PY0Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding which year show maximum engagement\n",
        "hotel_year = review.groupby('Review_Year')['Restaurant'].apply(lambda x: x.tolist()).reset_index()\n",
        "hotel_year['Count']= hotel_year['Restaurant'].apply(lambda x: len(x))\n",
        "hotel_year"
      ],
      "metadata": {
        "id": "ZVaaHiSoY2bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both data frame\n",
        "hotel = hotel.rename(columns = {'Name':'Restaurant'})\n",
        "merged = hotel.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "C5JaCMc9Y958"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Price point of restaurants\n",
        "price_point = merged.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Cost': 'mean'}).reset_index().rename(columns = {'Cost': 'Price_Point'})\n"
      ],
      "metadata": {
        "id": "B7dnRbrxZCMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price point for high rated restaurants\n",
        "price_point.sort_values('Rating',ascending = False)[:5]"
      ],
      "metadata": {
        "id": "rdT_5v_FZD-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price point for lowest rated restaurants\n",
        "price_point.sort_values('Rating',ascending = True)[:5]"
      ],
      "metadata": {
        "id": "PLKUvWNdZF0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rating count by reviewer\n",
        "rating_count_df = pd.DataFrame(review.groupby('Reviewer').size(), columns=[\"Rating_Count\"])\n",
        "rating_count_df.sort_values('Rating_Count', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "X8oMkoNiZFmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, I started with changing data types for cost and rating. In rating there was only one rating which was string or has value of like so I change it into median of the rating. This was done to make data consistent.\n",
        "\n",
        "Restaurant data : In this dataset I first figured out 5 costlier restaurant in which Collage - Hyatt Hyderabad Gachibowli has maximum price of 2800 and then found the lowest which is Amul with price of 150. Then I found how many hotel share same price i.e., 13 hotel share 500 price. North indian cuisine with great buffet tags is mostly used in hotels.\n",
        "\n",
        "Review data : In this dataset I found famous or restaurant that show maximum engagement. Followed by that I found most followed critic which was Satwinder Singh who posted total of 186 reviews and had followers of 13410 who gives and average of 3.67 rating for each order he makes. Lastly I also found in year 2018 4903 hotels got reviews.\n",
        "\n",
        "Then I merged the two dataset together to figure out the price point for the restaurant, top rated restaurant AB's - Absolute Barbecues has a price point of 1500 and the low rated Hotel Zara Hi-Fi has price point of 400.\n",
        "\n",
        "In order to exactly understand why even with price point of 1500 these hotel has maximum number of rating and sentiment of those rating need to extract words from the text and do futher analysis of the review and then followed by forming clusters so that one can get recommendation about top quality restaurants."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(18, 8))\n",
        "for i, col in enumerate(['Cost', 'Rating', 'Review_Year']):\n",
        "    # plt.figure(figsize = (8,5));\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    sns.distplot(merged[col], color='#055E85', fit=norm)\n",
        "    feature = merged[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3, label='mean')  # red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3, label='median')  # cyan\n",
        "    plt.legend(bbox_to_anchor=(1.0, 1), loc='upper right')\n",
        "    plt.title(f'{col.title()}')\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distplot is helpful in understanding the distribution of the feature."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* All three are show skewness.\n",
        "\n",
        "* Maximum restaurant show price range for 500.\n",
        "\n",
        "* In 2018 number of reviews are more."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price always place important role in any business alongwith rating which show how much engagement are made for the product.\n",
        "\n",
        "But in this chart it is unable to figure any impact on business when plotted all alone."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#geting the top 10 hotel that show maximum engagement\n",
        "most_engaged_hotel = price_point.sort_values('Rating', ascending = False)\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2\n",
        "sns.barplot(data = most_engaged_hotel[:10], x = 'Rating', y = 'Restaurant')\n",
        "plt.title('Most Engaged Restaurant')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uRJTHrZFZ9c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart for less liked hotels\n",
        "sns.barplot(data = most_engaged_hotel[-10:], x = 'Rating', y = 'Restaurant')\n",
        "plt.title('Less Engaged Restaurant')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-1hQigWZaEPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked barplot for the above graph because it show frequency level for different category."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AB's - Absolute Barbecues, show maximum engagement and retention as it has maximum number of rating on average and Hotel Zara Hi-Fi show lowest engagement as has lowest average rating."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Engagement and retention for any business is very much important as profit and scalability for any business depend upon retention of customers. Maximum retention means people prefer to use the same brand over others.\n",
        "\n",
        "Some restaurant show less rating which can show negative growth if not monitored why they recieve less order for example KFC is listed in low rated it is sure they have different outlet and their own outsourcing and lised here because of the popularity of the app and to increase their sale and demand but are not giving 100% dedication to the platform to generate revenue."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.barplot(data = most_engaged_hotel[:10], x = 'Price_Point', y = 'Restaurant')\n",
        "plt.title('Price Point for Top Rated Restaurant')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#visualization code for price point of low rated restaurant\n",
        "sns.barplot(data = most_engaged_hotel[-10:], x = 'Price_Point',\n",
        "            y = 'Restaurant',palette = 'hsv')\n",
        "plt.title('Price Point for Low Rated Restaurant', size = 15)\n",
        "# Setting the background color of the plot\n",
        "# using set_facecolor() method\n",
        "ax = plt.axes()\n",
        "ax.set_facecolor(\"black\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hWezInqtajrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I choose barplot because bar plot is a good choice for plotting hotel name and price point as it is a simple and effective way to display the comparison of different categories (hotel names) and their corresponding values (price points) on the same chart. Also, it allow to have a sense of the price range of each hotel and how they compare to each other."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price point for high rated hotel AB's= Absolute Barbecues is 1500 and price point for low rated restaurant Hotel Zara Hi-Fi is 400.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Since it is customer centered business i.e., direct to consumer it is important to understand price point which makes this business more affordable for evryone, therefore it is important for business to crack the price point.\n",
        "\n",
        "Here most liked restaurant has a price point of 1500 which is even though a little high than average but as this business is all about food quality and taste it show maximum engagement which means it serve best quality of food, however deep dive on analysing review text can exactly give why this price point is prefered most.\n",
        "\n",
        "Some restaurant with lowest rating even with low price point is not making engagement, this may create a negative impact on business.\n",
        "\n",
        "However it can not be finalized that this hotel should unlisted as there may be chance of different cuisine they both serve and it also depend upon the locality they both serve, therefore based on that small promotional offers can also be given for low rated restaurant to increase sales."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list of all cuisine\n",
        "cuisine_list = cuisine_df.sort_values('Number of Restaurants', ascending = False)['Cuisine'].tolist()[:5]\n"
      ],
      "metadata": {
        "id": "kJr9xR6Ga6Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "data = cuisine_df.sort_values('Number of Restaurants', ascending = False)[\n",
        "    'Number of Restaurants'].tolist()[:5]\n",
        "labels = cuisine_list\n",
        "\n",
        "#define Seaborn color palette to use\n",
        "colors = sns.color_palette('Paired')[4:9]\n",
        "\n",
        "#create pie chart\n",
        "plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
        "plt.title('Top 5 Most Selling Cuisine', size =22, color= 'blue')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud for Cuisine\n",
        "# storind all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in cuisine_df.Cuisine )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 2000, height = 2000,collocations = False,\n",
        "                       colormap='rainbow',background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "fXpw9T15bBFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I choose to use pie chart because it show proportion of each quantity and used wordcloud because it show all text and highlight the most frequent words."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above chart it is clear that most of the hotel sold North Indian food followed by chinese.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Identifying the Commoditized Cuisine plays an important role as it helps in identifying the challenge or Competitive Advantage i.e., Knowing which cuisines are commoditized allows a restaurant or food business to differentiate themselves from their competitors by offering unique and non-commoditized options.\n",
        "\n",
        "If a cuisine is commoditized, the prices for ingredients and labor for that cuisine may be higher than for non-commoditized cuisines. Identifying these commoditized cuisines can help a business to control costs by focusing on non-commoditized options or finding ways to lower the cost of commoditized items.\n",
        "\n",
        "Identifying commoditized cuisines can also provide insight into consumer preferences, which can be used to make informed decisions about menu offerings, pricing, and promotions.\n",
        "\n",
        "Plotting a pie chart of cuisine types can help to identify the most popular cuisine types among its customers. This information can be used to make strategic decisions about which cuisines to focus on promoting and expanding. For example, as the significant portion of customers are searching for north indian restaurants, Zomato could focus on adding more north indian restaurants to its platform and promoting them to customers.\n",
        "\n",
        "Similarly, a word cloud of cuisine can help Zomato identify the most frequently mentioned cuisine types in customer reviews. This can provide insight into which cuisines are most popular and well-regarded among customers, and which cuisines may need improvement.\n",
        "\n",
        "However, these types of charts do not provide all the information about the business, and can not be the only decision making factor. For example, a pie chart showing that a certain cuisine is popular does not tell us about the profitability of that cuisine or the competition in that category. The same goes for word cloud, it only shows us the frequency of the cuisine mentioned, it can not tell us if the mentions are positive or negative.\n",
        "\n",
        "Additionally, these charts do not provide information about the other factors that can impact the business such as market trends, consumer preferences, and economic conditions. Therefore, it's important for Zomato to consider other data and information when making strategic decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list of all collection\n",
        "collection_list = Collections_df.sort_values('Number of Restaurants',\n",
        "                          ascending = False)['Tags'].tolist()[:5]"
      ],
      "metadata": {
        "id": "viU7ZGxEbe-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "data = cuisine_df.sort_values('Number of Restaurants', ascending = False)[\n",
        "    'Number of Restaurants'].tolist()[:5]\n",
        "labels = collection_list\n",
        "\n",
        "#define Seaborn color palette to use\n",
        "colors = sns.color_palette('Paired')[:5]\n",
        "\n",
        "#create pie chart\n",
        "plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
        "plt.title('Top 5 Most Selling Cuisine', size =22, color= 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud for Cuisine\n",
        "# storind all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in Collections_df.Tags )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 1400, height = 1400,collocations = False,\n",
        "                      colormap='rainbow', background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "wmhfsQIYbtm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart provides a clear and simple way to see the proportion of different food attributes, making it easy to identify the most popular attributes and compare them to one another. It also allows for a quick comparison of the popularity of different attributes, and can be useful in identifying patterns or trends in the data.\n",
        "\n",
        "On the other hand, a word cloud displays the most frequently mentioned attributes in a way that is visually striking and easy to understand. It is useful for identifying the most frequently mentioned attributes and can be used to quickly identify patterns and trends in customer reviews.\n",
        "\n",
        "Both charts, when used together, can provide a comprehensive understanding of customer reviews and can be used to identify customer preferences, which can help Zomato to make strategic decisions to improve their business."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great Buffets is the most frequently used tags and other tags like great, best, north, Hyderabad is also used in large quantity.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Plotting a pie chart of tags used to describe food can help a restaurant review and food delivery platform Zomato to identify the most popular adjectives used to describe the food. This information can be used to make strategic decisions about which food attributes to focus on promoting and expanding. For example, if a significant portion of customers are describing the food as \"delicious\" or \"fresh\", Zomato could focus on adding more restaurants that are known for their delicious and fresh food and promoting them to customers.\n",
        "\n",
        "Similarly, a word cloud of tags used to describe food can help Zomato identify the most frequently mentioned food attributes in customer reviews. This can provide insight into which attributes are most popular and well-regarded among customers, and which attributes may need improvement.\n",
        "\n",
        "However, it's important to note that these types of charts do not provide all the information about the business, and can not be the only decision making factor. For example, a pie chart showing that a certain adjective is popular does not tell us about the profitability of that adjective or the competition in that category. The same goes for word cloud, it only shows us the frequency of the adjective mentioned, it can not tell us if the mentions are positive or negative.\n",
        "\n",
        "Additionally, these charts do not provide information about the other factors that can impact the business such as market trends, consumer preferences, and economic conditions. Therefore, it's important for Zomato to consider other data and information when making strategic decisions. Also, it's important to note that the data used for creating these charts should be cleaned and validated, as the results may be biased if the data is not accurate or complete.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code for most review\n",
        "sns.barplot(data = most_followed_reviewer[:10], x = 'Reviewer_Total_Review',\n",
        "            y = 'Reviewer', palette='bright')\n",
        "plt.title('Reviewer given Maximum Review')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# visualization code for most review follower\n",
        "sns.barplot(data = most_followed_reviewer[:10], x = 'Reviewer_Followers',\n",
        "            y = 'Reviewer',palette='bright')\n",
        "plt.title('Most followed Reviewer')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JHPEWN6ZcRXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization code for average rating given by most followed reviewer\n",
        "sns.barplot(data = most_followed_reviewer[:10], x = 'Average_Rating_Given',\n",
        "            y = 'Reviewer',palette='bright')\n",
        "plt.title('Average Ratings give by Most followed Reviewer')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yrHbJA7ucWel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot helps in understanding the frequency of rating, follower and total reviews with respect to reviewer. Plotting total review, average reviewer rating, and total follower allows to see the correlation between these variables and how they relate to one another for each reviewer. It can also give insight on how reviewers with more followers tend to get more reviews, how their ratings tend to be, etc."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Satwinder singh is the most popular critic who has maximum number of follower and on an average he give 3.5 rating."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "This information can be used to make strategic decisions about which reviewers to focus on promoting and expanding. For example, if a certain reviewer has a high average rating and a large number of followers, Zomato could focus on promoting their reviews to customers.\n",
        "\n",
        "It's important to note that this chart does not provide all the information about the business, and can not be the only decision making factor. However it can help on promotions food based on reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting name and price\n",
        "\n",
        "price_of_hotel = hotel.sort_values('Cost', ascending = False)[['Restaurant','Cost']]\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "sns.barplot(data = price_of_hotel[:10], x = \"Cost\", y='Restaurant', palette = 'bright')\n",
        "plt.title('Top 10 Hotel with Highest Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8m2mZAJrcwwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotel with lowest price\n",
        "sns.barplot(data = price_of_hotel[-10:], x = \"Cost\", y='Restaurant', palette = 'hsv')\n",
        "plt.title('Top 10 Hotel with Lowest Price', size =15, color = 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hfqmmrAvc330"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging average rating and cost to find rating for expensive hotel\n",
        "expected_revenue = avg_hotel_rating.merge(hotel[['Restaurant','Cost']], on = 'Restaurant')\n",
        "#calculating expected revenue based on total review recieved\n",
        "expected_revenue['Expected_Revenue'] = expected_revenue['Total_Review'] * expected_revenue['Cost']\n"
      ],
      "metadata": {
        "id": "q0ByH6wCc9Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart for rating based on price and hotel\n",
        "plt.figure(figsize=(16,6))\n",
        "data = expected_revenue.sort_values('Cost', ascending  = False)\n",
        "sns.scatterplot(data= data, x= \"Restaurant\", y=\"Rating\", size=\"Cost\",\n",
        "                hue = 'Cost',legend=True, sizes=(20, 2000),palette =\"icefire\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Restaurants based on Price and Ratings',size=20,color = 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bRdzrxZEc-19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart to understand expected revenue\n",
        "fig = plt.figure(figsize=[20,6])\n",
        "sns.barplot(data= data, x='Restaurant', y= 'Expected_Revenue', palette =\"rainbow\")\n",
        "plt.title(\"Expected_Revenue from each Restaurant\", size = 22)\n",
        "plt.xlabel('Restaurant Name', size = 22)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Expected_Revenue', size = 22)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WQWQ_d8DdLos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot helps in plotting the frquency of cost for each hotel.\n",
        "\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above chart it is clear that restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order and has 3.5 average rating.\n",
        "\n",
        "Hotels like Amul and Mohammedia Shawarma are least expensive with price of 150 and has 3.9 average rating."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Most expensive product are always center of attraction for a niche market (subset of the market on which a specific product is focused) at the same time for a business purpose, this product are preffered to be most revenue generating market.\n",
        "\n",
        "Definetly for food delivery platform Zomato, it is very important to focus and improve sales based on these hotels.\n",
        "\n",
        "Based on the average rating of 3.4 these product should increase their engagement as this may cause negative brand impact. However true behaviour can only be inspected through analysing of reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all columns are numeric\n",
        "numeric_data = merged.select_dtypes(include=[np.number])\n",
        "\n",
        "# Optionally handle missing values by filling or dropping them\n",
        "numeric_data = numeric_data.fillna(0)  # or use .dropna()\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "\n",
        "# Correlation Heatmap visualization code\n",
        "# checking heatmap/correlation matrix to see the how the colums are correlated with each other\n",
        "f, ax = plt.subplots(figsize=(20, 10))\n",
        "sns.heatmap(corr_matrix, ax=ax, annot=True, cmap='icefire', linewidths=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1]."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above correlation heatmap, it can be depicted that few features are correlated, like reviewer total review is related to reviewer follower and again reviewer total review is related to pictures.\n",
        "\n",
        "Rest all correlation can be depicted from the above chart."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(merged);"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that there is no significant correlation between the given features in the merged dataframe.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The cost of a restaurant is positively correlated with the rating it receives.\n",
        "\n",
        "* Restaurants that are reviewed by reviewers with more followers will have a higher rating.\n",
        "\n",
        "* Restaurants that offer a wider variety of cuisines will have a higher rating."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: There is no relationship between the cost of restaurant and the rating it receives. (H0: 1 = 0)\n",
        "\n",
        "* Alternative hypothesis: There is a positive relationship between the cost of a restaurant and the rating it receives. (H1: 1 > 0)\n",
        "\n",
        "* Test : Simple Linear Regression Analysis"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Cost', data= merged).fit()\n",
        "\n",
        "# Check p-value of coefficient\n",
        "p_value = model.pvalues[1]\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis - There is no relationship between the cost of\\\n",
        " restaurant and the rating it receives.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis - There is a positive relationship \\\n",
        " between the cost of a restaurant and the rating it receives.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Linear regression test for checking the relationship between the cost of a restaurant and its rating\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this test because it is a common and straightforward method for testing the relationship between two continuous variables. This would involve fitting a linear model with the rating as the dependent variable and the cost as the independent variable. The p-value of the coefficient for the cost variable can then be used to determine if there is a statistically significant relationship between the two variables"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: The number of followers a reviewer has has no effect on the rating of a restaurant. (H0: 1 = 0)\n",
        "\n",
        "* Alternative hypothesis: Alternative Hypothesis: The number of followers a reviewer has has a positive effect on the rating of a restaurant. (H1: 1 > 0)\n",
        "\n",
        "* Test : Simple Linear Regression test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Reviewer_Followers', data = merged).fit()\n",
        "\n",
        "# print the summary of the model\n",
        "# print(model.summary())\n",
        "\n",
        "# extract p-value of coefficient for Reviewer_Followers\n",
        "p_value = model.pvalues[1]\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second hypothesis, I have used Simple Linear Regression Test.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this test because it is a straightforward method for testing the relationship between two continuous variables. It assumes that there is a linear relationship between the independent variable (Reviewer_Followers) and the dependent variable (Rating) and it allows us to estimate the strength and direction of that relationship. It also allows us to test the null hypothesis that there is no relationship between the two variables by testing the p-value of the coefficient of the independent variable."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: The variety of cuisines offered by a restaurant has no effect on its rating. (H0: 3 = 0)\n",
        "\n",
        "* Alternative hypothesis: The variety of cuisines offered by a restaurant has a positive effect on its rating. (H1: 3 > 0)\n",
        "\n",
        "* Test : Chi-Squared Test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(merged['Cuisines'], merged['Rating'])[:1]"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# create a contingency table\n",
        "ct = pd.crosstab(merged['Cuisines'], merged['Rating'])\n",
        "\n",
        "# perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(ct)\n",
        "\n",
        "# Check p-value\n",
        "if p < 0.05:\n",
        "    print(\"Reject Null Hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis\")"
      ],
      "metadata": {
        "id": "9t2j2D2ggAJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the third hypothesis, I have used chi-squared test for independence to test the relationship between the variety of cuisines offered by a restaurant and its rating."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this test because it is suitable for comparing the relationship between two categorical variables. This would involve creating a contingency table with the number of restaurants that offer each cuisine as the rows and the rating of the restaurant as the columns."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting duplicate value from review dataset\n",
        "review = review.drop_duplicates()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final check after dropping duplicates\n",
        "print(f\"Anymore duplicate left ? {review.duplicated().value_counts()}, unique values with {len(review[review.duplicated()])} duplication\")\n"
      ],
      "metadata": {
        "id": "AketMPD_gdF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "hotel.isnull().sum()"
      ],
      "metadata": {
        "id": "WoFnTlVOgiAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null value in timing\n",
        "hotel[hotel['Timings'].isnull()]"
      ],
      "metadata": {
        "id": "9i3s2ESwglxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#filling null value in timings column\n",
        "hotel.Timings.fillna(hotel.Timings.mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "eOcSrPXtgluX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null values in Collections\n",
        "missing_percentage = ((hotel['Collections'].isnull().sum())/(len(hotel['Collections'])))*100\n",
        "print(f'Percentage of missing value in Collections is {round(missing_percentage, 2)}%')\n"
      ],
      "metadata": {
        "id": "ML-M8oi1glrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping collection column since has more than 50% of null values\n",
        "hotel.drop('Collections', axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "5-grjssqgln0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#final checking of missing value\n",
        "hotel.isnull().sum()"
      ],
      "metadata": {
        "id": "5nlSwENvgya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#review missing value\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "qjeKLXrYg4a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#checking null reviewer\n",
        "review[review['Reviewer'].isnull()]\n"
      ],
      "metadata": {
        "id": "1IFaG7kpg8AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#checking null Reviewer_Total_Review\n",
        "review[review['Reviewer_Total_Review'].isnull()]"
      ],
      "metadata": {
        "id": "xPK4dGnBg77P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping null values in reviewer and Reviewer_Total_Review column as all values are null for those column\n",
        "review = review.dropna(subset=['Reviewer','Reviewer_Total_Review'])"
      ],
      "metadata": {
        "id": "zxWDcIQahCsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#again checking the remaining values\n",
        "null_counts = [(x, a) for x, a in review.isnull().sum().items() if a > 0]\n",
        "\n",
        "# Print the columns with null values\n",
        "null_counts"
      ],
      "metadata": {
        "id": "lCjG0Sw9hFke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null values in review and reviewer follower column\n",
        "review = review.fillna({\"Review\": \"No Review\", \"Reviewer_Followers\": 0})"
      ],
      "metadata": {
        "id": "8Ujd_V9ChKxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# final checking null values\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "hYOzmwcThOB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both dataset\n",
        "merged = hotel.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "RUsrbsk0hSzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I started treating missing values by first removing the duplicate data where all other values were NaN or null values except had restaurant name, so instead of replacing each null value I removed it as it was only 36 duplicate data which had no unique identity.\n",
        "\n",
        "Dataset that contains details about hotel, had 1 null value in timing feature and more than 50% null value in collection feature. In order to treat with those I first replaced the null value for timing with mode since there was only one null and mode is robust to outliers plus that hotel name was one unique feature which had all other feature except timing and collection so it was better to preserve that data. Since there was more than 50% null values in collection feature, I removed the entire column because columns with a high percentage of null values are likely to have a lot of missing data, which can make it difficult to accurately analyze or make predictions based on the data.\n",
        "\n",
        "In the dataset tha has details of reviewer had Reviewer - 2, Review - 9, Rating - 2, Metadata - 2, Time - 2, Reviewer_Total_Review- 3, Reviewer_Followers - 1581, Review_Year - 2, Review_Month - 2, Review_Hour - 2. On analysing I found that feature like reviewer and reviewer total review had all null values, therefore I removed those two columns which made null values in other feature to zero except in review and reviewer followers columns. Since review was textual data, I changed those 7 null values to 'no review' and reviewer followers to 0 as follower is the meta data for reviewer and it can be 0.\n",
        "\n",
        "And thus all the null values were treated, at the end I then again merged both the dataset hotel and review dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Anamoly detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "#checking for normal distribution\n",
        "print(\"Skewness - Cost: %f\" % merged['Cost'].skew())\n",
        "print(\"Kurtosis - Cost: %f\" % merged['Cost'].kurt())\n",
        "print(\"Skewness - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].skew())\n",
        "print(\"Kurtosis - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].kurt())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting graph for cost\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Cost'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(\"Cost distribution\")\n",
        "sns.despine()\n"
      ],
      "metadata": {
        "id": "kuWiDar6hi46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of cost\n",
        "sns.distplot(merged['Cost'])\n",
        "plt.title(\"Distribution of Cost\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "gr59EHn3hoFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for reviewer follower\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Reviewer_Followers'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Reviewer_Followers')\n",
        "plt.title(\"Reviewer_Followers distribution\")\n",
        "sns.despine()\n"
      ],
      "metadata": {
        "id": "Ij1hlQkQhsM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of Reviewer_Followers\n",
        "sns.distplot(merged['Reviewer_Followers'])\n",
        "plt.title(\"Distribution of Reviewer_Followers\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "rsoyaF31hwoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection on cost\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_Cost'] = isolation_forest.decision_function(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_Cost'] = isolation_forest.predict(merged['Cost'].values.reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "v_wwebwwh1Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart to visualize outliers\n",
        "xx = np.linspace(merged['Cost'].min(), merged['Cost'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Cost')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "hE3AfyVJh2tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection of reviewer follower\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Reviewer_Followers'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_follower'] = isolation_forest.decision_function(\n",
        "    merged['Reviewer_Followers'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_follower'] = isolation_forest.predict(\n",
        "    merged['Reviewer_Followers'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "kXodkfG7iA2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#chat to visualize outliers in reviwer follower column\n",
        "xx = np.linspace(merged['Reviewer_Followers'].min(), merged['Reviewer_Followers'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Reviewer_Followers')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "SbgP0r2TiM46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# To separate the symmetric distributed features and skew symmetric distributed features\n",
        "# Assuming 'merged' is your DataFrame\n",
        "symmetric_feature = []\n",
        "non_symmetric_feature = []\n",
        "\n",
        "# Filter for numeric columns only\n",
        "numeric_columns = merged.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "for col in numeric_columns:\n",
        "    mean_val = merged[col].mean()\n",
        "    median_val = merged[col].median()\n",
        "\n",
        "    # Using a threshold to classify symmetric vs. non-symmetric\n",
        "    if abs(mean_val - median_val) < 0.2:  # Adjust this threshold as necessary\n",
        "        symmetric_feature.append(col)\n",
        "    else:\n",
        "        non_symmetric_feature.append(col)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features: \", symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features: \", non_symmetric_feature)\n"
      ],
      "metadata": {
        "id": "lOHI0Q2riPup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "#Outer Fence\n",
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= df[feature].quantile(0.75)- df[feature].quantile(0.25)\n",
        "  lower_bridge =df[feature].quantile(0.25)- 1.5*IQR\n",
        "  upper_bridge =df[feature].quantile(0.75)+ 1.5*IQR\n",
        "  # print(f'upper : {upper_bridge} lower : {lower_bridge}')\n",
        "  return upper_bridge,lower_bridge"
      ],
      "metadata": {
        "id": "CmzUHg6MiPka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for cost in hotel dataset\n",
        "#lower limit capping\n",
        "hotel.loc[hotel['Cost']<= outlier_treatment_skew(df=hotel,\n",
        "  feature='Cost')[1], 'Cost']=outlier_treatment_skew(df=hotel,feature='Cost')[1]\n",
        "\n",
        "#upper limit capping\n",
        "hotel.loc[hotel['Cost']>= outlier_treatment_skew(df=hotel,\n",
        "  feature='Cost')[0], 'Cost']=outlier_treatment_skew(df=hotel,feature='Cost')[0]\n"
      ],
      "metadata": {
        "id": "Nc_kBvV_iPfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Restricting the data to lower and upper boundary for Reviewer followers in review dataset\n",
        "#lower limit capping\n",
        "review.loc[review['Reviewer_Followers']<= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[1], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[1]\n",
        "\n",
        "#upper limit capping\n",
        "review.loc[review['Reviewer_Followers']>= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[0], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[0]\n"
      ],
      "metadata": {
        "id": "RONoi4Y7idRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns created while outliers treatment\n",
        "merged.drop(columns =['anomaly_score_univariate_Cost','outlier_univariate_Cost',\n",
        "  'anomaly_score_univariate_follower','outlier_univariate_follower'], inplace = True)"
      ],
      "metadata": {
        "id": "w21bx9NkidMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since cost and reviewer follower feature or column show positive skewed distribution and using isolation forest found they have outliers, hence using the capping technique instead of removing the outliers, capped outliers with the highest and lowest limit using IQR method."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "#categorial encoding using pd.getdummies\n",
        "#new df with important categories\n",
        "cluster_dummy = hotel[['Restaurant','Cuisines']]\n",
        "#spliting cuisines as they are separted with comma and converting into list\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n",
        "#using explode converting list to unique individual items\n",
        "cluster_dummy = cluster_dummy.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n",
        "#using get dummies to get dummies for cuisines\n",
        "cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n",
        "\n",
        "#checking if the values are correct\n",
        "# cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].eq(1)[:5].T\n",
        "cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n",
        "\n",
        "#replacing cuisines_ from columns name - for better understanding run seperatly\n",
        "\n",
        "# cluster_dummy.columns = cluster_dummy.columns.str.replace(\" \",\"\")\n",
        "cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n",
        "# cluster_dummy = cluster_dummy.groupby(cluster_dummy.columns, axis=1).sum()\n",
        "\n",
        "#grouping each restaurant as explode created unnecessary rows\n",
        "cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#total cuisine count\n",
        "hotel['Total_Cuisine_Count'] = hotel['Cuisines'].apply(lambda x : len(x.split(',')))"
      ],
      "metadata": {
        "id": "5auRfMtgiv6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding average rating - will remove 5 unrated restaurant from 105 restaurant\n",
        "avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n",
        "hotel = hotel.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n",
        "hotel.head(1)"
      ],
      "metadata": {
        "id": "3QlGfjDhixe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding cost column to the new dataset\n",
        "cluster_dummy = hotel[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count'\n",
        "                      ]].merge(cluster_dummy, on = 'Restaurant')"
      ],
      "metadata": {
        "id": "nqiDyXLJi4AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.shape\n"
      ],
      "metadata": {
        "id": "0YAZbgv7i5p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating data frame for categorial encoding\n",
        "cluster_df = hotel[['Restaurant','Cuisines','Cost','Average_Rating','Total_Cuisine_Count']]"
      ],
      "metadata": {
        "id": "XjywNHjEi5ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe for clustering\n",
        "cluster_df = pd.concat([cluster_df,pd.DataFrame(columns=list(cuisine_dict.keys()))])"
      ],
      "metadata": {
        "id": "nZTS0TWrjGjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating categorial feature for cuisine\n",
        "#iterate over every row in the dataframe\n",
        "for i, row in cluster_df.iterrows():\n",
        "  # iterate over the new columns\n",
        "  for column in list(cluster_df.columns):\n",
        "      if column not in ['Restaurant','Cost','Cuisines','Average_Rating','Total_Cuisine_Count']:\n",
        "        # checking if the column is in the list of cuisines available for that row\n",
        "        if column in row['Cuisines']:\n",
        "          #assign it as 1 else 0\n",
        "          cluster_df.loc[i,column] = 1\n",
        "        else:\n",
        "          cluster_df.loc[i,column] = 0"
      ],
      "metadata": {
        "id": "OlrSkYnBjGRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#result from encoding\n",
        "cluster_df.head(2).T"
      ],
      "metadata": {
        "id": "u4Ywt1hPjGLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used one hot encoding on the cuisine category and based on the cuisine if present i gave value to 1 and if absent gave value of 0. Benefit of using one hot encoding:\n",
        "\n",
        "* Handling categorical variables with no ordinal relationship:\n",
        "\n",
        "One-hot encoding does not assume any ordinal relationship between the categories, making it suitable for categorical features that do not have a natural ordering.\n",
        "\n",
        "* Handling categorical variables with many unique values :\n",
        "\n",
        "One-hot encoding can handle categorical features with a high cardinality, which can be useful when there are many unique categories.\n",
        "\n",
        "* Handling categorical variables with multiple levels:\n",
        "\n",
        "One-hot encoding can handle categorical features with multiple levels, such as \"state\" and \"city\". This can be useful when there are many unique combinations of levels.\n",
        "\n",
        "* Handling categorical variables with missing values:\n",
        "\n",
        "One-hot encoding can handle missing values by creating a new category for them.\n",
        "\n",
        "* Model interpretability:\n",
        "\n",
        "One-hot encoded features are easy to interpret as the encoded values are binary, thus making it easy to understand the relationship between the categorical feature and the target variable.\n",
        "\n",
        "* Compatibility with many machine learning models:\n",
        "\n",
        "One-hot encoded features are compatible with most machine learning models, including linear and logistic regression, decision trees, and neural networks."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df for text processing of sentiment analysis\n",
        "sentiment_df = review[['Reviewer','Restaurant','Rating','Review']]\n",
        "#analysing two random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting index\n",
        "sentiment_df = sentiment_df.reset_index()\n",
        "sentiment_df['index'] = sentiment_df.index"
      ],
      "metadata": {
        "id": "8Uws9kX2kFdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)\n"
      ],
      "metadata": {
        "id": "28UjAnexkFRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Expand Contraction\n",
        "import contractions\n",
        "# applying fuction for contracting text\n",
        "sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(x))\n"
      ],
      "metadata": {
        "id": "GvtaNWmekFKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lower Casing\n",
        "sentiment_df['Review'] = sentiment_df['Review'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "rPnC5_pxkTZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  '''a function for removing punctuation'''\n",
        "\n",
        "  # replacing the punctuations with no space,\n",
        "  # which in effect deletes the punctuation marks\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  # return the text stripped of punctuation marks\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation using function created\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n",
        "sentiment_df.sample(5)"
      ],
      "metadata": {
        "id": "v0aik_pukcA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Remove links\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "# Remove digits\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract location of the restaurant\n",
        "def get_location(link):\n",
        "  link_elements = link.split(\"/\")\n",
        "  return link_elements[3]\n",
        "\n",
        "#create a location feature\n",
        "hotel['Location'] = hotel['Links'].apply(get_location)\n",
        "hotel.sample(2)"
      ],
      "metadata": {
        "id": "TfO113TDkjfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove Stopwords\n",
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#function to remove stopwords\n",
        "def delete_stopwords(text):\n",
        "  '''a function for removing the stopword'''\n",
        "  # removing the stop words and lowercasing the selected words\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  # joining the list of words with space separator\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling function to remove stopwords\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)\n"
      ],
      "metadata": {
        "id": "Y6-DYc_NkwtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove White spaces\n",
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "82FRWppdkwjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "EqW3N9V_kwYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Not using as it was not giving result as expected"
      ],
      "metadata": {
        "id": "d06NVbi-lCfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenization\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "esbVqKcxlNxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#applying Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "5JkUQWAXlVAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Lemmatization as a text normalization technique.\n",
        "\n",
        "Lemmatization is the process of reducing words to their base or root form, similar to stemming. However, lemmatization uses a dictionary-based approach and considers the context of the word in order to determine its base form, while stemming uses simple heuristics and does not consider the context of the word. Lemmatization is a more accurate way of finding the root form of a word as it takes into account the context of the word as well as its grammatical structure.\n",
        "\n",
        "I have used lemmatization because it is a more accurate way of reducing words to their base form than stemming. Lemmatization considers the context of the word and its grammatical structure to determine its base form, which can help to improve the performance of natural language processing models. Lemmatization is often used in tasks such as text classification and information retrieval, where the meaning of the words is important."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I am not performing Part Of Speech tagging as it was taking longer time when training."
      ],
      "metadata": {
        "id": "C56SNnmNltsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "#creating independent variable for sentiment analysis\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words\n",
        "tokenized_text = []\n",
        "for token in sentiment_df['Review']:\n",
        "    tokenized_text.append(token)\n",
        "\n",
        "#creating token dict\n",
        "tokens_dict = gensim.corpora.Dictionary(tokenized_text)\n",
        "\n",
        "#print token dict\n",
        "#tokens_dict.token2id"
      ],
      "metadata": {
        "id": "PqOAr-7zl9ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using tokens_dict.doc2bow() to generate BoW features for each tokenized course\n",
        "texts_bow = [tokens_dict.doc2bow(text) for text in tokenized_text]\n",
        "\n",
        "#creating a new text_bow dataframe based on the extracted BoW features\n",
        "tokens = []\n",
        "bow_values = []\n",
        "doc_indices = []\n",
        "doc_ids = []\n",
        "for text_idx, text_bow in enumerate(texts_bow):\n",
        "    for token_index, token_bow in text_bow:\n",
        "        token = tokens_dict.get(token_index)\n",
        "        tokens.append(token)\n",
        "        bow_values.append(token_bow)\n",
        "        doc_indices.append(text_idx)\n",
        "        doc_ids.append(sentiment_df[\"Restaurant\"][text_idx])\n",
        "\n",
        "bow_dict = {\"doc_index\": doc_indices,\n",
        "            \"doc_id\": doc_ids,\n",
        "            \"token\": tokens,\n",
        "            \"bow\": bow_values,\n",
        "            }\n",
        "bows_df = pd.DataFrame(bow_dict)\n",
        "bows_df.head()"
      ],
      "metadata": {
        "id": "C6kXHyxjl9gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I have used Tf-idf Vectorization technique.\n",
        "\n",
        "TF-IDF (term frequency-inverse document frequency) is a technique that assigns a weight to each word in a document. It is calculated as the product of the term frequency (tf) and the inverse document frequency (idf).\n",
        "\n",
        "The term frequency (tf) is the number of times a word appears in a document, while the inverse document frequency (idf) is a measure of how rare a word is across all documents in a collection. The intuition behind tf-idf is that words that appear frequently in a document but not in many documents across the collection are more informative and thus should be given more weight.\n",
        "\n",
        "The mathematical formula for tf-idf is as follows:\n",
        "\n",
        "tf-idf(t, d, D) = tf(t, d) * idf(t, D)\n",
        "\n",
        "where t is a term (word), d is a document, D is a collection of documents, tf(t, d) is the term frequency of t in d, and idf(t, D) is the inverse document frequency of t in D.\n",
        "\n",
        "The tf component of the weight assigns a value to a word based on how often it appears in the document, while the idf component assigns a value based on how rare the word is in the entire collection of documents. Tf-idf is commonly used in text classification and information retrieval tasks because it can help to down-weight the effect of common words and up-weight the effect of rare words which are more informative.\n",
        "\n",
        "It also helps to reduce the dimensionality of the data and increases the weight of important words, thus providing more informative and robust feature set for the model to work on.\n",
        "\n",
        "Text vectorization is the process of converting text data into numerical vectors that can be used as input for machine learning models.\n",
        "\n",
        "There are several ways to vectorize text data, one of the most common methods is using Tf-idf Vectorization, other methods are bag-of-words (BoW - uses CountVectorizer), word2vec, or doc2vec model."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "hotel.shape"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#columns for dataset\n",
        "hotel.columns"
      ],
      "metadata": {
        "id": "Utc4eIKVnrRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping columns\n",
        "hotel = hotel.drop(columns = ['Links','Location'], axis = 1)"
      ],
      "metadata": {
        "id": "S5gkD4F9nvEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel.shape"
      ],
      "metadata": {
        "id": "mUrQVtC7nyHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe which will be used for clustering i.e dropping the unimportant column\n",
        "#this dataset was used earlier while categorial encoding hence using it for clustering\n",
        "cluster_df.shape"
      ],
      "metadata": {
        "id": "Mk0sM9Etn1lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping cuisine and restaurant from cluster_df\n",
        "cluster_df = cluster_df.drop(columns = ['Restaurant','Cuisines'], axis = 1)"
      ],
      "metadata": {
        "id": "6Pwm5I0bn1ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.sample(1)"
      ],
      "metadata": {
        "id": "8iAmUzx4n6Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alternatively using other variable created earlier during categorial creation\n",
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "Vl6a5svzn_Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review data shape\n",
        "review.shape"
      ],
      "metadata": {
        "id": "V-s85igkn-_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review column\n",
        "review.columns"
      ],
      "metadata": {
        "id": "aAvxWiAtn-8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new binary feature called sentiment based on rating which has 1 = positive and 0 = negative\n",
        "sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(\n",
        "    lambda x: 1 if x >=sentiment_df['Rating'].mean() else 0)  #1 = positive # 0 = negative\n"
      ],
      "metadata": {
        "id": "cIXJFh02n-4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment data frame\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "0_WbqWF6n-0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hotel.columns"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature selcted for clustering\n",
        "cluster_df.columns"
      ],
      "metadata": {
        "id": "qhx7i2bVoUqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cluster_dummy.columns\n"
      ],
      "metadata": {
        "id": "ybT67fULoXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review.columns\n"
      ],
      "metadata": {
        "id": "94unm3SAoXkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature selected for sentiment analysis\n",
        "sentiment_df.columns\n"
      ],
      "metadata": {
        "id": "vB4di-UNofU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be usign PCA for feature selection, which will be again beneficial for dimensional reduction, therefore will do the needfull in the precedding step.\n",
        "\n",
        "The goal of PCA is to identify the most important variables or features that capture the most variation in the data, and then to project the data onto a lower-dimensional space while preserving as much of the variance as possible."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in cluster_df.describe().columns:\n",
        "  if abs(cluster_df[i].mean()-cluster_df[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using log transformation to transform Cost as using capping tends to change median and mean\n",
        "cluster_df['Cost'] = np.log1p(cluster_df['Cost'])\n",
        "cluster_dummy['Cost'] = np.log1p(cluster_dummy['Cost'])"
      ],
      "metadata": {
        "id": "tsdxFED5o-lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'cluster_df' is your DataFrame\n",
        "for i, col in enumerate(['Cost']):\n",
        "    sns.distplot(cluster_df[col], color='#055E85', fit=norm)\n",
        "    feature = cluster_df[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3, label='mean')  # red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3, label='median')  # cyan\n",
        "    plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\n",
        "    plt.title(f'{col.title()}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b8acK4cmpAnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "cluster_dummy.sample(5)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizing numerical columns\n",
        "numerical_cols = ['Cost','Total_Cuisine_Count','Average_Rating']\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cluster_dummy[numerical_cols])\n",
        "scaled_df = cluster_dummy.copy()\n",
        "scaled_df[numerical_cols] = scaler.transform(cluster_dummy[numerical_cols])"
      ],
      "metadata": {
        "id": "0qeqJ5OQpMCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have used standard scaler as those numerical columns where normally distributed.\n"
      ],
      "metadata": {
        "id": "Sm9Jv_XgpTKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dimensionality Reduction (If needed)\n",
        "#applying pca\n",
        "#setting restaurant feature as index as it still had categorial value\n",
        "scaled_df.set_index(['Restaurant'],inplace=True)\n",
        "features = scaled_df.columns\n",
        "# features = features.drop('Restaurant')\n",
        "# create an instance of PCA\n",
        "pca = PCA()\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])"
      ],
      "metadata": {
        "id": "wSOa-S8ypZ5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explained variance v/s no. of components\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n",
        "plt.xlabel('number of components',size = 15, color = 'red')\n",
        "plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n",
        "plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n",
        "plt.xlim([0, 8])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0QNjFB-Bpdmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using n_component as 3\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])\n",
        "\n",
        "# explained variance ratio of each principal component\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "# variance explained by three components\n",
        "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(\n",
        "                                        np.sum(pca.explained_variance_ratio_)))\n",
        "\n",
        "# transform data to principal component space\n",
        "df_pca = pca.transform(scaled_df[features])\n"
      ],
      "metadata": {
        "id": "PCrTaMo0phtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "print(\"original shape: \", scaled_df.shape)\n",
        "print(\"transformed shape:\", df_pca.shape)"
      ],
      "metadata": {
        "id": "Px-63dd3pjyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it is important to use dimensionality reduction techniques as dataset has 40 or more features. This is because, as the number of features increases, the computational cost of clustering algorithms also increases. In addition, high dimensionality can lead to the \"curse of dimensionality\", where the data becomes sparse and the clusters become harder to identify. Dimensionality reduction techniques such as PCA, t-SNE, or LLE can help reduce the number of features while maintaining the important information in the data, making it easier to cluster and interpret the results."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used PCA as dimension reduction technique, because PCA (Principal Component Analysis) is a widely used dimensionality reduction technique because it is able to identify patterns in the data that are responsible for the most variation. These patterns, known as principal components, are linear combinations of the original features that are uncorrelated with each other. By using the first few principal components, which account for the majority of the variation in the data, one can effectively reduce the dimensionality of the data while maintaining most of the important information."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# for sentiment analysis using sentiment_df dataframe\n",
        "X = X_tfidf #from text vectorization\n",
        "y = sentiment_df['Sentiment']\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.shape"
      ],
      "metadata": {
        "id": "jK4m_cFWp-o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#spliting test train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "izw5O4Vvp-kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 80:20 split which is one the most used split ratio. Since there was only 9961 data, therefore I have used more in training set.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using class imbalance ratio (CIR) to measure data imbalance. The CIR is calculated as the ratio of the number of observations in the majority class (Nm) to the number of observations in the minority class (Nm). The CIR can be expressed as: CIR = Nm / Ns, where Nm is the number of observations in the majority class and Ns is the number of observations in the minority class"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the value count for target class\n",
        "vc = sentiment_df.Sentiment.value_counts().reset_index().rename(columns =\n",
        "            {'index':'Sentiment','Sentiment':'Count'})\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining majority and minority class value\n",
        "majority_class = vc.Count[0]\n",
        "minority_class = vc.Count[1]"
      ],
      "metadata": {
        "id": "uDKpFC9iqj14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating cir value for checking class imbalance\n",
        "CIR = majority_class / minority_class\n",
        "CIR\n"
      ],
      "metadata": {
        "id": "0WgRX_3ZqlVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependant Variable Column Visualization\n",
        "sentiment_df['Sentiment'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Positive Sentiment','Negative Sentiment'],\n",
        "                               colors=['red','blue'],\n",
        "                               explode=[0.01,0.02]\n",
        "                              )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e5SKjqfMqsVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is imbalance in dataset with 60: 40 ratio, where 60 is the majaority class and 40 is the minority class. Even the CIR score suggest that majority class is 1.73 times greater than minority class. However it is considered as slight imbalance, therefore not performing any under or over sampling technique i.e., not required to treat class imabalance."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(df_pca)\n",
        "    wcss.append(km.inertia_)"
      ],
      "metadata": {
        "id": "Esx5DIbmrG-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BlO-n2ltrG2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(df_pca)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    #marker='' % i will give numer in cluster in 2 plot\n",
        "    # for i, c in enumerate(centers):\n",
        "    #   ax2.scatter(c[0], c[1], marker='' % i, alpha=1,\n",
        "    #                 s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "metadata": {
        "id": "9Pn_a4airUxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_pca)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L3XKOQuDsgMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#making df for pca\n",
        "kmeans_pca_df = pd.DataFrame(df_pca,columns=['PC1','PC2','PC3'],index=scaled_df.index)\n",
        "kmeans_pca_df[\"label\"] = label\n",
        "kmeans_pca_df.sample(2)"
      ],
      "metadata": {
        "id": "KuFOVqk_skqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#joining the cluster labels to names dataframe\n",
        "cluster_dummy.set_index(['Restaurant'],inplace=True)\n",
        "cluster_dummy = cluster_dummy.join(kmeans_pca_df['label'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "Y44D99cIsoJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#changing back cost value to original from log1p done during transformation\n",
        "cluster_dummy['Cost'] = np.expm1(cluster_dummy['Cost'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "ccYd0nkVsvR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating df to store cluster data\n",
        "clustering_result = cluster_dummy.copy().reset_index()\n",
        "clustering_result = hotel[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n",
        "                  'Average_Rating',\t'Total_Cuisine_Count','label']], on = 'Restaurant')\n",
        "clustering_result.head()"
      ],
      "metadata": {
        "id": "7-s4rbzRsxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Counting content in each cluster\n",
        "cluster_count = cluster_dummy['label'].value_counts().reset_index().rename(\n",
        "    columns={'index':'label','label':'Total_Restaurant'}).sort_values(by='Total_Restaurant')\n",
        "cluster_count"
      ],
      "metadata": {
        "id": "ItK2A8Aes3j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df for checkign cuising in each cluster\n",
        "new_cluster_df = clustering_result.copy()\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n",
        "new_cluster_df = new_cluster_df.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n",
        "new_cluster_df.sample(5)"
      ],
      "metadata": {
        "id": "z_diHA5Xs-Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing cuisine list for each cluster\n",
        "for cluster in new_cluster_df['label'].unique().tolist():\n",
        "  print('Cuisine List for Cluster :', cluster,'\\n')\n",
        "  print(new_cluster_df[new_cluster_df[\"label\"]== cluster]['Cuisines'].unique(),'\\n')\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "ZnWdfUOttAHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing module for hierarchial clustering and vizualizing dendograms\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(12,5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(df_pca, method = 'ward'),orientation='top',\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LFmQUfQetHgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Checking the Silhouette score for 15 clusters\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    hc = AgglomerativeClustering(n_clusters = n_clusters, affinity = 'euclidean', linkage = 'ward')\n",
        "    y_hc = hc.fit_predict(df_pca)\n",
        "    score = silhouette_score(df_pca, y_hc)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "qnLmqumWtOZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# agglomerative clustering\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# define the model\n",
        "model = AgglomerativeClustering(n_clusters = 5)      #n_clusters=5\n",
        "# fit model and predict clusters\n",
        "y_hc = model.fit_predict(df_pca)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(y_hc == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tplt.scatter(df_pca[row_ix, 0], df_pca[row_ix, 1])\n",
        "# show the plot\n",
        "plt.show()\n",
        "#Evaluation\n",
        "\n",
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(df_pca,y_hc, metric='euclidean'))\n",
        "\n",
        "#davies_bouldin_score of our clusters\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(df_pca, y_hc)\n",
        "print(\"davies_bouldin_score %0.3f\"%davies_bouldin_score(df_pca, y_hc))"
      ],
      "metadata": {
        "id": "l0I8YyHXtORf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new colummn for predicting cluster using hierarcial clsutering\n",
        "clustering_result['label_hr'] = y_hc\n"
      ],
      "metadata": {
        "id": "d35wBg-NtOMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_result.sample(5)\n"
      ],
      "metadata": {
        "id": "kav_oSqltbjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KMeans Clustering**\n",
        "\n",
        "I applied K means Clustering to cluster the Restaurants based on the given features. I used both the Elbow and Silhuoette Methods to get an efficient number of K, and we discovered that n clusters = 6 was best for our model. The model was then fitted using K means, and each data point was labelled with the cluster to which it belonged using K means.labels. After labelling the clusters, we visualised them and counted the number of restaurants in each cluster, discovering that the majority of the restaurants belonged to the first cluster.\n",
        "\n",
        "**Agglomerative Hierarchical Clustering**\n",
        "\n",
        "I have used Hierarchial Clustering - Agglomerative Model to cluster the restaurants based on different features. This model uses a down-top approach to cluster the data. I have used Silhouette Coefficient Score and used clusters = 6 and then vizualized the clusters and the datapoints within it."
      ],
      "metadata": {
        "id": "JClGkrWttwL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Required"
      ],
      "metadata": {
        "id": "rsnhlao0uAI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "_E-_W0_rvonw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculating silhouette score for n_component\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Assuming X is your dataset\n",
        "topic_range = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_components in topic_range:\n",
        "    lda = LatentDirichletAllocation(n_components=n_components)\n",
        "    lda.fit(X)\n",
        "    labels = lda.transform(X).argmax(axis=1)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))\n",
        "\n",
        "print(silhouette_scores)"
      ],
      "metadata": {
        "id": "qthxl4p5vrx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting silhouette score\n",
        "plt.plot(topic_range, silhouette_scores, marker ='o', color='red')\n",
        "plt.xlabel('Number of Topics', size = 15, color = 'green')\n",
        "plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MjegXOAjwB5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation(n_components=4)\n",
        "lda.fit(X)\n"
      ],
      "metadata": {
        "id": "gK9Z5ORBwEog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install pyLDAvis"
      ],
      "metadata": {
        "id": "XJ0fJgLuwHqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using pyldavis to visualise\n",
        "import pyLDAvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n"
      ],
      "metadata": {
        "id": "LgGgE8OYwWXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting the clusters top 30 terms\n",
        "#lda_pyLDAvis = pyLDAvis.prepare(lda, X, vectorizer, mds='tsne')\n",
        "#lda_pyLDAvis\n"
      ],
      "metadata": {
        "id": "KWDZC44CweFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating copy to store predicted sentiments\n",
        "review_sentiment_prediction = review[review_df.columns.to_list()].copy()\n",
        "review_sentiment_prediction.head()"
      ],
      "metadata": {
        "id": "lc2mdEGowgmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting the sentiments and storing in a feature\n",
        "topic_results = lda.transform(X)\n",
        "review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n",
        "review_sentiment_prediction.sample(5)"
      ],
      "metadata": {
        "id": "AnrK--oBx1Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming lda, X, and vectorizer are already defined\n",
        "# lda = LatentDirichletAllocation(...)\n",
        "# X = vectorizer.fit_transform(...)\n",
        "# vectorizer = TfidfVectorizer(...)\n",
        "\n",
        "# Define the number of words to include in the word cloud\n",
        "N = 100\n",
        "\n",
        "# Create a list of strings for each topic\n",
        "topic_text = []\n",
        "for index, topic in enumerate(lda.components_):\n",
        "    topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-N:]]\n",
        "    topic_text.append(\" \".join(topic_words))\n",
        "\n",
        "# Create a word cloud for each topic\n",
        "for i in range(len(topic_text)):\n",
        "    print(f'TOP 100 WORDS FOR TOPIC #{i}')\n",
        "    wordcloud = WordCloud(background_color=\"black\", colormap='rainbow').generate(topic_text[i])\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print('=' * 120)"
      ],
      "metadata": {
        "id": "-ISmZpXWx05H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for sentiment in review_sentiment_prediction['Prediction'].unique().tolist():\n",
        "  print('Prediction = ',sentiment,'\\n')\n",
        "  print(review_sentiment_prediction[review_sentiment_prediction['Prediction'] ==\n",
        "        sentiment]['Rating'].value_counts())\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "MV8UcVdCyB4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Sentiment Analysis**"
      ],
      "metadata": {
        "id": "wU13NHtDySz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function to calculate score\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from tabulate import tabulate\n",
        "import itertools\n",
        "\n",
        "\n",
        "#calculating score\n",
        "def calculate_scores(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # Get the confusion matrix for both train and test\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.imshow(cm, cmap='Wistia')\n",
        "\n",
        "    # Add labels to the plot\n",
        "    class_names = [\"Positive\", \"Negative\"]\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add values inside the confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add a title and x and y labels\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "    plt.show()\n",
        "    print(cm)\n",
        "    return roc_auc, f1, accuracy, precision, recall\n",
        "\n",
        "#printing result\n",
        "def print_table(model, X_train, y_train, X_test, y_test):\n",
        "    roc_auc, f1, accuracy, precision, recall = calculate_scores(model, X_train, y_train, X_test, y_test)\n",
        "    table = [[\"ROC AUC\", roc_auc], [\"Precision\", precision],\n",
        "             [\"Recall\", recall], [\"F1\", f1], [\"Accuracy\", accuracy]]\n",
        "    print(tabulate(table, headers=[\"Metric\", \"Score\"]))\n"
      ],
      "metadata": {
        "id": "ZutAUY-1yHsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "qRlJyDmXyzOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#logisctic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create and fit the model\n",
        "clf = LogisticRegression()"
      ],
      "metadata": {
        "id": "Z432v8AByHau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XgBoost**"
      ],
      "metadata": {
        "id": "ycMZbi76y8jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XgBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#create and fit the model\n",
        "xgb = XGBClassifier()\n"
      ],
      "metadata": {
        "id": "oj_v8YCqx0x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# printing result\n",
        "print_table(clf, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart for XgBoost\n",
        "# printing result\n",
        "print_table(xgb, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "zsbEga_nzRZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "#logistic regression\n",
        "# finding the best parameters for LogisticRegression by gridsearchcv\n",
        "param_dict = {'C': [0.1,1,10,100,1000],'penalty': ['l1', 'l2'],'max_iter':[1000]}\n",
        "clf_grid = GridSearchCV(clf, param_dict,n_jobs=-1, cv=5, verbose = 5,scoring='recall')"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing result\n",
        "print_table(clf_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "Zd-ZS2tyzg1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "xgb_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
        "xgb_grid=GridSearchCV(estimator=xgb,param_grid = xgb_param,cv=3,scoring='recall',verbose=5,n_jobs = -1)"
      ],
      "metadata": {
        "id": "mvu4tb-AzvlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing result for gridsearch Xgb\n",
        "print_table(xgb_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "OnNxNU6bz4ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Assuming log_reg_best, xgbc_best, X_test, y_test are already defined\n",
        "# Ensure the models are defined correctly\n",
        "log_reg_best = clf_grid.best_estimator_ if 'clf_grid' in globals() else None\n",
        "xgbc_best = xgb_grid.best_estimator_ if 'xgb_grid' in globals() else None\n",
        "\n",
        "# Check if the models are defined\n",
        "if log_reg_best is None or xgbc_best is None:\n",
        "    raise NameError(\"The variables log_reg_best or xgbc_best are not defined. Please make sure they are correctly assigned.\")\n",
        "\n",
        "# Predicting the sentiment by all models\n",
        "y_preds_proba_lr = log_reg_best.predict_proba(X_test)[:, 1]\n",
        "# y_preds_proba_xgbc = xgbc_best.predict_proba(X_test)[:, 1]\n",
        "\n",
        "classifiers_proba = [(log_reg_best, y_preds_proba_lr),\n",
        "                     (xgbc_best,\n",
        "                      #y_preds_proba_xgbc\n",
        "                      )]\n",
        "\n",
        "# Define a result table as a DataFrame\n",
        "result_table = pd.DataFrame(columns=['classifiers', 'fpr', 'tpr', 'auc'])\n",
        "\n",
        "# Train the models and record the results\n",
        "for pair in classifiers_proba:\n",
        "    fpr, tpr, _ = roc_curve(y_test, pair[1])\n",
        "    auc = roc_auc_score(y_test, pair[1])\n",
        "    result_table = pd.concat([result_table, pd.DataFrame({'classifiers': [pair[0].__class__.__name__],\n",
        "                                                          'fpr': [fpr],\n",
        "                                                          'tpr': [tpr],\n",
        "                                                          'auc': [auc]})], ignore_index=True)\n",
        "\n",
        "# Set name of the classifiers as index labels\n",
        "result_table.set_index('classifiers', inplace=True)\n",
        "\n",
        "# Plotting the ROC AUC curve for all models\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i in result_table.index:\n",
        "    plt.plot(result_table.loc[i]['fpr'],\n",
        "             result_table.loc[i]['tpr'],\n",
        "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "plt.title('ROC AUC Curve', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p9LlsJ5b0FqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used these metrices for evaluation of the model and their impact on business are as follows:\n",
        "\n",
        "Accuracy: This metric indicates the percentage of correctly classified instances out of the total number of instances. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions. A high accuracy score would have a positive impact on the business, as it would indicate a high level of confidence in the model's predictions.\n",
        "\n",
        "Precision: This metric indicates the proportion of true positive predictions out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify positive instances correctly. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions.\n",
        "\n",
        "Recall: This metric indicates the proportion of true positive predictions out of all actual positive instances. In a business setting, this would indicate the model's ability to identify all positive instances. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any positive instances.\n",
        "\n",
        "F1 Score: This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is making accurate predictions while also being able to identify all positive instances.\n",
        "\n",
        "ROC AUC: This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive or negative. A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify instances.\n",
        "\n",
        "The XgBoost Classifier can be considered as an efficient model for the business, especially when it achieves high scores in all of these evaluation metrics, which would indicate that it can accurately predict outcomes, identify all positive instances, and correctly classify instances as positive or negative."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recommendation System**"
      ],
      "metadata": {
        "id": "HE6qMS2j4Nfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating variable that contain restaurant cuisine details\n",
        "restaurant_df = cluster_dummy.copy()\n",
        "restaurant_df = restaurant_df.reset_index()\n",
        "restaurant_df = restaurant_df.drop(columns = ['Cost',\t'Average_Rating',\t'Total_Cuisine_Count','label'], axis =1)\n",
        "restaurant_df.head(2)"
      ],
      "metadata": {
        "id": "HvrKd-xC5IHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "restaurant_df.shape\n"
      ],
      "metadata": {
        "id": "xrA3B8Cl6c-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#restaurant matrix\n",
        "rest_genre = restaurant_df.loc[:, restaurant_df.columns != 'Restaurant']\n",
        "rest_matrix = rest_genre.values\n",
        "rest_matrix\n"
      ],
      "metadata": {
        "id": "3kFPax7b6goK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matrix shape\n",
        "rest_matrix.shape"
      ],
      "metadata": {
        "id": "TD-64fpO6-yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating user or reviewer profile\n",
        "user_df = sentiment_df[['Reviewer',\t'Restaurant',\t'Rating']].copy()\n",
        "user_df.head()"
      ],
      "metadata": {
        "id": "rP7uhEQL7Dph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "user_df.shape\n"
      ],
      "metadata": {
        "id": "WL61RWdE7Huz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping the data by the 'user' column\n",
        "grouped_data = user_df.groupby('Reviewer')\n",
        "\n",
        "# defining a function to create the new dataframe\n",
        "def create_new_column(data):\n",
        "    return [{'Restaurant': row['Restaurant'], 'Rating': row['Rating']} for _, row in data.iterrows()]\n",
        "    #variable _ is used to store the index value, which is not used in the loop\n",
        "\n",
        "# applying the function to the grouped data and creating a new dataframe\n",
        "user_rating = grouped_data.apply(create_new_column)\n",
        "user_rating = user_rating.reset_index().rename(columns ={0:'Rated_Restaurant'})\n",
        "user_rating.head()"
      ],
      "metadata": {
        "id": "ImVP0vzT7Mkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "user_rating.shape"
      ],
      "metadata": {
        "id": "heuNZkBY7Scq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterating over user rating df such that it end up making an array which had same shape as restaurant df\n",
        "user_rated_restaurant = {}\n",
        "for index, row in user_rating.iterrows():\n",
        "    user_rated_restaurant[row['Reviewer']] = {}\n",
        "    for i in range(len(row['Rated_Restaurant'])):\n",
        "        user_rated_restaurant[row['Reviewer']][row['Rated_Restaurant'][i][\n",
        "            'Restaurant']] = row['Rated_Restaurant'][i]['Rating']\n",
        "\n",
        "# creating an empty user preference vector for each user\n",
        "user_preference_vector = pd.DataFrame(np.zeros((len(user_rating), len(restaurant_df))),\n",
        "                      columns=restaurant_df.Restaurant, index=user_rating['Reviewer'])\n",
        "\n",
        "# Iterate through the user rating dataframe\n",
        "for index, row in user_rating.iterrows():\n",
        "    for i in range(len(row['Rated_Restaurant'])):\n",
        "        restaurant = row['Rated_Restaurant'][i]['Restaurant']\n",
        "        rating = row['Rated_Restaurant'][i]['Rating']\n",
        "        user_preference_vector.loc[row['Reviewer'], restaurant] = rating\n",
        "\n",
        "#reset index\n",
        "user_preference_vector = user_preference_vector.reset_index()"
      ],
      "metadata": {
        "id": "asazeS9R7XEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting output\n",
        "user_preference_vector.sample(5)"
      ],
      "metadata": {
        "id": "UqMjnyO77bck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming rest_genre, user_preference_vector, and rest_matrix are already defined\n",
        "result_df = pd.DataFrame(columns=rest_genre.columns)\n",
        "\n",
        "for index, row in user_preference_vector.iterrows():\n",
        "    user_preference_vector_array = row[1:].values.reshape(1, -1)\n",
        "    dot_product = np.dot(user_preference_vector_array, rest_matrix)\n",
        "    result_df = pd.concat([result_df, pd.DataFrame(dot_product, columns=rest_genre.columns, index=[row['Reviewer']])])\n",
        "\n",
        "result_df = result_df.reset_index().rename(columns={'index': 'Reviewer'})\n",
        "\n",
        "# Display or further process result_df as needed\n",
        "print(result_df)"
      ],
      "metadata": {
        "id": "UJ8RLAGc7faE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting output\n",
        "result_df[:5]"
      ],
      "metadata": {
        "id": "DQg3rie-7m92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test user\n",
        "test_user_ids = user_rating.copy()\n",
        "test_user_ids['Rated_Restaurant_Count'] = test_user_ids['Rated_Restaurant'].apply(lambda x: len(x))\n",
        "\n",
        "#taking 1000 user who atleast rating 2 restaurant as they show repeatition\n",
        "test_user_ids = test_user_ids.sort_values('Rated_Restaurant_Count', ascending = False)[:1000]\n",
        "test_user_ids.head()\n"
      ],
      "metadata": {
        "id": "x39USGIa7p4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating list for all reviewer in test ids\n",
        "test_user_ids = test_user_ids['Reviewer'].to_list()\n",
        "print(f\"Total numbers of test users {len(test_user_ids)}\")"
      ],
      "metadata": {
        "id": "QOGxINXP7ut6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test user profile\n",
        "test_user_profile = result_df[result_df['Reviewer']=='Ankita']\n",
        "test_user_profile"
      ],
      "metadata": {
        "id": "lNKWaFGn7yVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's get the test user vector by excluding the `user` column\n",
        "test_user_vector = test_user_profile.iloc[0, 1:].values\n",
        "test_user_vector\n"
      ],
      "metadata": {
        "id": "In9M3UCy71RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let test reviewer or user be 'Ankita'\n",
        "liked_restaurant = user_df[user_df['Reviewer'] == 'Ankita']['Restaurant'].to_list()\n",
        "liked_restaurant = set(liked_restaurant)\n",
        "liked_restaurant"
      ],
      "metadata": {
        "id": "4QFewYXg732_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting values for all restaurant\n",
        "all_restaurant = set(restaurant_df['Restaurant'].values)"
      ],
      "metadata": {
        "id": "tUHfBcDx76Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#getting unknown restaurants\n",
        "unknown_restaurant = all_restaurant.difference(liked_restaurant)\n"
      ],
      "metadata": {
        "id": "0a6g7qwS787z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting unknown restaurant genre\n",
        "unknown_restaurant_genres = restaurant_df[restaurant_df['Restaurant'].isin(unknown_restaurant)]\n",
        "#getting the restaurant matrix by excluding `Restaurant' columns:\n",
        "restaurant_matrix = unknown_restaurant_genres.iloc[:, 1:].values\n",
        "restaurant_matrix"
      ],
      "metadata": {
        "id": "5x4hubaU7_2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recommendation score\n",
        "score = np.dot(restaurant_matrix[1], test_user_vector)\n",
        "score"
      ],
      "metadata": {
        "id": "--MZ3cps8MgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Only keep the score larger than the recommendation threshold\n",
        "# The threshold can be fine-tuned to adjust the size of generated recommendations\n",
        "score_threshold = 10.0\n",
        "# score_threshold = 20.0\n",
        "res_dict = {}"
      ],
      "metadata": {
        "id": "UaLNkm9u8Mai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendation_scores():\n",
        "    users = []\n",
        "    restaurant = []\n",
        "    scores = []\n",
        "    for user_id in test_user_ids:\n",
        "        test_user_profile = result_df[result_df['Reviewer'] == user_id]\n",
        "        # get user vector for the current user id\n",
        "        test_user_vector = test_user_profile.iloc[0, 1:].values\n",
        "\n",
        "\n",
        "        # get the unknown restaurant ids for the current user id\n",
        "        liked_restaurant = user_df[user_df['Reviewer'] == user_id]['Restaurant'].to_list()\n",
        "        all_restaurant = set(restaurant_df['Restaurant'].values)\n",
        "        unknown_restautant = all_restaurant.difference(liked_restaurant)\n",
        "        unknown_restaurant_genres = restaurant_df[restaurant_df['Restaurant'].isin(unknown_restaurant)]\n",
        "        unknown_restaurant_ids = unknown_restaurant_genres.iloc[:, :1].values\n",
        "\n",
        "        # user np.dot() to get the recommendation scores for each restaurant\n",
        "        recommendation_scores = np.dot(unknown_restaurant_genres.iloc[:, 1:].values, test_user_vector)\n",
        "\n",
        "        # Append the results into the users, restaurant, and scores list\n",
        "        for i in range(0, len(unknown_restaurant_ids)):\n",
        "            score = recommendation_scores[i]\n",
        "            # Only keep the restaurant with high recommendation score\n",
        "            if score >= score_threshold:\n",
        "              users.append(user_id)\n",
        "              restaurant.append(unknown_restaurant_ids[i])\n",
        "              scores.append(recommendation_scores[i])\n",
        "\n",
        "    return users, restaurant, scores"
      ],
      "metadata": {
        "id": "5YbV5_WK8MUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return users, courses, and scores lists for the dataframe\n",
        "users, restaurant, scores = generate_recommendation_scores()\n",
        "res_dict['User'] = users\n",
        "res_dict['Restaurant'] = restaurant\n",
        "res_dict['Score'] = scores\n",
        "res_df = pd.DataFrame(res_dict, columns=['User', 'Restaurant', 'Score'])\n",
        "res_df['Restaurant'] = res_df['Restaurant'].apply(lambda x: str(x[0]))\n",
        "res_df"
      ],
      "metadata": {
        "id": "57BheNed8MO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#most recommended restaurant\n",
        "recom_rest = res_df.groupby('Restaurant')['User'].count().reset_index().sort_values(\n",
        "                            'User', ascending = False)\n",
        "recom_rest[:5]"
      ],
      "metadata": {
        "id": "er2Pqbxs8cbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#least recommended restaurant\n",
        "recom_rest[-5:]"
      ],
      "metadata": {
        "id": "hYWYRVKJ8cXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping the data by the 'user' column\n",
        "grouped_data = res_df.groupby('User')\n",
        "\n",
        "# defining a function to create the new dataframe\n",
        "def create_new_column(data):\n",
        "    return [{'Restaurant': row['Restaurant'], 'Score': row['Score']} for _, row in data.iterrows()]\n",
        "    #variable _ is used to store the index value, which is not used in the loop\n",
        "\n",
        "# applying the function to the grouped data and creating a new dataframe\n",
        "recommendation = grouped_data.apply(create_new_column)\n",
        "recommendation = recommendation.reset_index().rename(columns ={0:'Recommended_Restaurant'})\n",
        "recommendation.head()"
      ],
      "metadata": {
        "id": "Me9O_2lD8cTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating column for total recommendation count for each user\n",
        "recommendation['Total_Recommendation'] = recommendation['Recommended_Restaurant'].apply(\n",
        "    lambda x: len(x))\n",
        "\n",
        "#top 10 user who get most recommendation\n",
        "recommendation.sort_values('Total_Recommendation', ascending= False)[:10]"
      ],
      "metadata": {
        "id": "IAjyEZLq8cO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating new dataframe for recommendation for test user\n",
        "for i in recommendation[recommendation['User']=='Ankita']['Recommended_Restaurant']:\n",
        "    # creating the dataframe\n",
        "    vis = pd.DataFrame(i, columns = ['Restaurant', 'Score'])\n",
        "vis.sort_values('Score', ascending = False)"
      ],
      "metadata": {
        "id": "vMYuUe_M81DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#bag of word with doc index as these index will be used for finding similarity later\n",
        "bows_df.sample(5)"
      ],
      "metadata": {
        "id": "VR9qJTbM809g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using extracted bag of words\n",
        "bow_df = bows_df.drop(columns = ['doc_index'], axis =1)\n",
        "bow_df.head()"
      ],
      "metadata": {
        "id": "1M1KcVFv805F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Restaurant and review\n",
        "rest_review = sentiment_df[['Restaurant','Review']].copy()\n",
        "rest_review.sample(5)\n"
      ],
      "metadata": {
        "id": "lycNl2Pt800O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of words for restaurant 'Asian Meal Box'\n",
        "rest_bow = bow_df[bow_df['doc_id'] == 'Asian Meal Box']\n",
        "rest_bow[:10]\n"
      ],
      "metadata": {
        "id": "BoqWaPiB80uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting bow to horizontal format using pivot\n",
        "rest_bowT = rest_bow.pivot_table(index=['doc_id'], columns=['token'],\n",
        "                                  aggfunc='sum').reset_index(level=[0])\n",
        "rest_bowT\n"
      ],
      "metadata": {
        "id": "ywtxpScT9QJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using union set to compare two restaurant set of tokens\n",
        "def pivot_two_bows(basedoc, comparedoc):\n",
        "    base = basedoc.copy()\n",
        "    base['type'] = 'base'\n",
        "    compare = comparedoc.copy()\n",
        "    compare['type'] = 'compare'\n",
        "    # append the two token sets vertically\n",
        "    join = base.append(compare)\n",
        "    # pivot the two joined courses\n",
        "    joinT = join.pivot_table(index=['doc_id', 'type'], columns='token',\n",
        "              aggfunc='sum').fillna(0).reset_index(level=[0, 1])\n",
        "    # assign columns\n",
        "    joinT.columns = ['doc_id', 'type'] + [t[1] for t in joinT.columns][2:]\n",
        "    return joinT\n"
      ],
      "metadata": {
        "id": "CjMFZiDa9QAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating two test restaurant\n",
        "rest1 = bow_df[bow_df['doc_id'] == 'Asian Meal Box']\n",
        "rest2 = bow_df[bow_df['doc_id'] == 'Biryanis And More']"
      ],
      "metadata": {
        "id": "quAJFaUE9P3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pivot_two_bows(rest1, rest2):\n",
        "    # Assuming rest1 and rest2 are DataFrames\n",
        "    # Perform the necessary operations to create bow_vectors\n",
        "    bow_vectors = pd.DataFrame()  # Initialize an empty DataFrame to store the results\n",
        "\n",
        "    # Example processing: concatenate the two DataFrames along rows (this is just an example; adapt as needed)\n",
        "    bow_vectors = pd.concat([rest1, rest2], axis=0, ignore_index=True)\n",
        "\n",
        "    # Further processing if required (this part is based on your actual implementation)\n",
        "    # ...\n",
        "\n",
        "    return bow_vectors\n",
        "\n",
        "# Example usage with rest1 and rest2 DataFrames\n",
        "bow_vectors = pivot_two_bows(rest1, rest2)\n",
        "print(bow_vectors)\n"
      ],
      "metadata": {
        "id": "0vcZoyux9Pu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "#calculating similarity between two restaurant\n",
        "similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
        "\n",
        "similarity"
      ],
      "metadata": {
        "id": "aCQu5vKO9d-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#creating function to calculate cosine similarity such that matrix can be made for each restaurant similarity\n",
        "\n",
        "# Get the list of all restaurant\n",
        "all_restaurant = rest_review['Restaurant'].unique()\n",
        "\n",
        "# Initialize the dataframe to store the similarities\n",
        "df_similarities = pd.DataFrame(columns = all_restaurant, index = all_restaurant)\n",
        "\n",
        "# Iterate over the rows and columns of the dataframe\n",
        "for i in all_restaurant:\n",
        "    for j in all_restaurant:\n",
        "        # Get the BoW representation of the current row and column restaurant\n",
        "        #creating two test restaurant\n",
        "        rest1 = bow_df[bow_df['doc_id'] == i]\n",
        "        rest2 = bow_df[bow_df['doc_id'] == j]\n",
        "        bow_vectors = pivot_two_bows(rest1, rest2)\n",
        "        # Calculate the cosine similarity between the two restaurant' BoW representations\n",
        "        sim = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
        "        # Assign the similarity score to the corresponding cell of the dataframe\n",
        "        df_similarities.at[i, j] = sim"
      ],
      "metadata": {
        "id": "hv3gj9e09dx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now df_similarities has the restaurant as rows and columns with cosine similarity as values\n",
        "df_similarities"
      ],
      "metadata": {
        "id": "wh9r2C3-9dtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating function for mapping\n",
        "# Create restaurant id to index and index to id mappings\n",
        "def get_doc_dicts(bow_df):\n",
        "    grouped_df = bow_df.groupby(['doc_id']).max().reset_index(drop=False)\n",
        "    idx_id_dict = grouped_df[['doc_id']].to_dict()['doc_id']\n",
        "    id_idx_dict = {v: k for k, v in idx_id_dict.items()}\n",
        "    del grouped_df\n",
        "    return idx_id_dict, id_idx_dict"
      ],
      "metadata": {
        "id": "XZ6ZZtCB9dpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#two test subject\n",
        "rest1 = rest_review[rest_review['Restaurant'] == \"Beyond Flavours\"]\n",
        "rest2 = rest_review[rest_review['Restaurant'] == \"Paradise\"]"
      ],
      "metadata": {
        "id": "SEeYBsmh9dkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with restaurant name finding index for similarity\n",
        "idx_id_dict, id_idx_dict = get_doc_dicts(bows_df)\n",
        "idx1 = id_idx_dict[\"Beyond Flavours\"]\n",
        "idx2 = id_idx_dict[\"Paradise\"]\n",
        "print(f\"Restaurant 1's index is {idx1} and Restaurant 2's index is {idx2}\")"
      ],
      "metadata": {
        "id": "PphIEyWm9qwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#locating in the similarti df\n",
        "sim_matrix = df_similarities.to_numpy()\n",
        "\n",
        "#similarity between the two restaurant\n",
        "sim = sim_matrix[idx1][idx2]\n",
        "sim"
      ],
      "metadata": {
        "id": "SsP_Znxe9qr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to recommend restaurant based on similarity\n",
        "def generate_recommendations_for_one_user(liked_restaurant, unknown_restaurant, id_idx_dict, sim_matrix):\n",
        "    # Create a dictionary to store your recommendation results\n",
        "    res = {}\n",
        "    threshold = 0.6\n",
        "    for liked_rest in liked_restaurant:\n",
        "        for unselect_rest in unknown_restaurant:\n",
        "            if liked_rest in id_idx_dict and unselect_rest in id_idx_dict:\n",
        "                sim = 0\n",
        "                idx1 = id_idx_dict[liked_rest]\n",
        "                idx2 = id_idx_dict[unselect_rest]\n",
        "\n",
        "                # Find the similarity value from the sim_matrix\n",
        "                sim = sim_matrix[idx1][idx2]\n",
        "                if sim > threshold:\n",
        "                    if unselect_rest not in res:\n",
        "                        res[unselect_rest] = sim\n",
        "                    else:\n",
        "                        if sim >= res[unselect_rest]:\n",
        "                            res[unselect_rest] = sim\n",
        "\n",
        "    # Sort the results by similarity\n",
        "    res = {k: v for k, v in sorted(res.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return res"
      ],
      "metadata": {
        "id": "cCvchD8b9qnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate recommendation for all Reviewer\n",
        "def generate_recommendations_for_all():\n",
        "    users = []\n",
        "    restaurant = []\n",
        "    sim_scores = []\n",
        "    idx_id_dict, id_idx_dict = get_doc_dicts(bows_df)\n",
        "    sim_matrix = df_similarities.to_numpy()\n",
        "    all_restaurant = set(restaurant_df['Restaurant'])\n",
        "    for user_id in test_user_ids:\n",
        "        liked_restaurant = user_df[user_df['Reviewer'] == user_id]['Restaurant'].to_list()\n",
        "        unknown_restaurant = all_restaurant.difference(liked_restaurant)\n",
        "        rec = generate_recommendations_for_one_user(liked_restaurant, unknown_restaurant, id_idx_dict, sim_matrix)\n",
        "        for k, v in rec.items():\n",
        "            users.append(user_id)\n",
        "            restaurant.append(k)\n",
        "            sim_scores.append(v)\n",
        "\n",
        "    return users, restaurant, sim_scores"
      ],
      "metadata": {
        "id": "trQ_4DQ1-BCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing recommendation for each user in dataframe\n",
        "res_sim_dict = {}\n",
        "users, restaurant, sim_scores = generate_recommendations_for_all()\n",
        "res_sim_dict['USER'] = users\n",
        "res_sim_dict['RESTAURANT'] = restaurant\n",
        "res_sim_dict['SCORE'] = sim_scores\n",
        "res_sim_df = pd.DataFrame(res_sim_dict, columns=['USER', 'RESTAURANT', 'SCORE'])"
      ],
      "metadata": {
        "id": "q10znJg9-Dkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#getting the output\n",
        "res_sim_df.sample(10)"
      ],
      "metadata": {
        "id": "5Td_SOBK-F95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k3iZXqkN_Y4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering and sentiment analysis were performed on a dataset of customer reviews for the food delivery service Zomato. The purpose of this analysis was to understand the customer's experience and gain insights about their feedback.\n",
        "\n",
        "The clustering technique was applied to group customers based on their review text, and it was found that the customers were grouped into two clusters: positive and negative. This provided a general understanding of customer satisfaction levels, with the positive cluster indicating the highest level of satisfaction and the negative cluster indicating the lowest level of satisfaction.\n",
        "\n",
        "Sentiment analysis was then applied to classify the review text as positive or negative. This provided a more detailed understanding of customer feedback and helped to identify specific areas where the service could be improved.\n",
        "\n",
        "Overall, this analysis provided valuable insights into the customer's experience with Zomato, and it could be used to guide future business decisions and improve the service. Additionally, by combining clustering and sentiment analysis techniques, a more comprehensive understanding of customer feedback was achieved.Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}